# PF_GOOGLE_YELP

<p align="center">
  <img src="./assets/logo.jpg" alt="Descripción de la imagen" width="600" height="600">
</p>

## Indice

<details open="open">
  <summary>Tabla de contenido: </summary>
  <ol>
    <li>
      <a href="#LATAMDATA">LATAMDATA</a>
      <ul>
        <li><a href="#Quienes-somos">¿Quienes somos?</a></li>
      </ul>
    </li>
    <li>
      <a href="#Planteamiento-del-problema">Planteamiento del problema</a>
    </li>
    <li>
      <a href="#Propuesta-de-proyecto">Propuesta de proyecto</a>
      <ul>
        <li><a href="#Objetivos">Objetivos</a></li>
        <li><a href="#kpis">KPIs</a></li>
        <li><a href="#Alcance">Alcance</a></li>
      </ul>
      <li><a href="#Solución Propuesta">Solución Propuesta</a></li>
      <ul>
        <li><a href="#Metodología-de-trabajo">Metodología de trabajo</a></li>
        <li><a href="#Estimación de Tiempos">Estimación de Tiempos</a></li>
        <li><a href="#Roles del Equipo">Roles del Equipo</a></li>
        <li><a href="#Stack-tecnologico">Stack Tecnológico</a></li>
        <li><a href="#Workflow">WorkFlow</a></li>
        <li><a href="#Pipeline">Pipeline</a></li>
      </ul>
      <ul>
        <li><a href="#Deploy ML">Deploy ML</a></li>
      </ul>
    </li> 
  </ol>
</details>

## LATAMDATA

### ¿Quienes somos?

Somos LATAM DATA CORPORATION, una consultora líder con sólida experiencia y vanguardia en diversos sectores industriales. Nuestro enfoque se centra en proporcionar a nuestros clientes soluciones avanzadas y personalizadas en análisis de datos, inteligencia empresarial y modelado predictivo. Colaboramos estrechamente con nuestros clientes para identificar las habilidades y conocimientos esenciales que serán cruciales en el panorama empresarial futuro.

## Planteamiento del problema

La empresa perteneciente al conglomerado hotelero que nos contrata cuenta con una trayectoria de décadas en la industria, han mantenido un compromiso firme con la satisfacción del cliente y la mejora continua, a pesar de su vasta experiencia en la industria hotelera, se encuentra en un momento de incursión en diversas subcategorías del sector. Esta expansión los ha llevado a explorar oportunidades en diferentes tipos de alojamiento, desde hoteles boutique hasta resorts de lujo y alojamientos temáticos. Esta amplia gama de opciones ha generado una mayor complejidad a la hora de comprender las preferencias y expectativas de los clientes en cada segmento. Por tanto, su interés radica en utilizar las opiniones y reseñas disponibles en plataformas como Google Maps y Yelp para orientar sus decisiones de inversión hacia las áreas que prometen un mayor potencial y demanda por parte de los usuarios. Por lo tanto, el desafío implica la recopilación, el procesamiento y la interpretación de grandes volúmenes de datos, así como la implementación de algoritmos inteligentes para ofrecer recomendaciones precisas y valiosas.

## Propuesta de proyecto

### Objetivos

1. Identificar las categorías de hoteles con potencial de mayor y menor crecimiento a través del análisis de opiniones en plataformas como Yelp y Google Maps

2. Detectar Sitios Óptimos para Nuevos Hoteles utilizando datos de opiniones para identificar las ubicaciones geográficas más estratégicas para la apertura de nuevos establecimientos.

3. Establecer un sistema de sugerencias basado en las preferencias y experiencias anteriores de los usuarios en ambas plataformas.

### KPIs

1. Tasa anual de satisfacción de los reviews: Calcula el porcentaje de calificaciones positivas por categoría hotelera que recibió. Es útil para medir la reputación de cada categoría y determinar la mejor.
2. Tasa anual de gastos en turismo: Calcula el porcentaje de gastos del turismo por estado. Es útil para ver el aumento del gasto en turismo.
3. Tasa de Crecimiento de Categoría Hotelera: Mide el crecimiento de cada categoría de la industria hotelera por estado con respecto al año anterior. Para determinar la categoría con mayor y menor crecimiento dependiendo del Estado.
4. Tasa de Ocurrencia: Mide la cantidad de reviews por estado y categorías con respecto al año anterior y así poder determinar el estado con mayor visitas por parte de los usuarios.

### Alcance

Conforme a los requerimientos del cliente, el proyecto se orienta hacia el ámbito turístico, centrándose específicamente en las categorías de alojamiento, con énfasis en hoteles, en los Estados Unidos. Dada la extensa diversidad estatal en este país, se ha llevado a cabo una investigación para identificar y diseñar estrategias acordes con los objetivos de la empresa contratante. Se ha determinado que, según datos recopilados por el Departamento de Comercio de los Estados Unidos (disponible en https://www-trade-gov.translate.goog/data-visualization/us-states-cities-visited-overseas-travelers?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=sc), los cinco estados más concurridos por turistas en el año 2022 fueron Florida, Nueva York, California, Nevada y Texas. Este análisis abarca el período desde el año 2016 hasta el 2022.

## Solución Propuesta

### Metodología de Trabajo

<p align="center">
  <img src="./assets/scrum.png" alt="Descripción de la imagen" width="400" height="400">
</p>

En el desarrollo de nuestro proyecto, hemos decidido implementar la metodología Scrum, reconocida por su capacidad para gestionar proyectos de manera ágil y eficiente. Scrum proporciona un marco de trabajo flexible que se adapta a las necesidades cambiantes del proyecto, permitiendo una respuesta rápida a los requisitos del cliente y facilitando la entrega continua de productos de alta calidad. Esta metodología se basa en la colaboración constante entre los miembros del equipo, la transparencia en la comunicación y la capacidad de adaptación a medida que evolucionan los objetivos del proyecto. Al adoptar Scrum, buscamos optimizar la productividad, mejorar la comunicación interna, y asegurar una entrega más rápida y efectiva de soluciones que cumplan con las expectativas del cliente. La planificación y revisión regular de sprints en Scrum nos permitirán mantener un enfoque iterativo y centrado en el valor, maximizando así la satisfacción del cliente y la eficiencia del equipo de desarrollo.

### Estimación de Tiempos

#### Sprint 1

<p align="center">
  <img src="./assets/gant1.jpg" alt="Descripción de la imagen" width="500" height="300">
</p>

#### Sprint 2

<p align="center">
  <img src="./assets/gan2.jpg" alt="Descripción de la imagen" width="500" height="300">
</p>

#### Sprint 3

<p align="center">
  <img src="./assets/gant3.jpg" alt="Descripción de la imagen" width="500" height="350">
</p>

Para la estimación de tiempo se elaboro un diagrama de gantt ubicado dentro de la carpeta assets/gant.pdf en el cual se establecio que tendra tres sprint con duración de una semana por cada sprint, del cual se tiene un comienzo el 2 de Enero y la finalización el 19 del mismo mes.
Cada Sprint cuenta con sus actividades y tareas para los miembros del equipo.

### Roles del Equipo

Nuestro equipo de expertos está constantemente actualizado en las últimas tendencias y tecnologías del mercado laboral para brindar soluciones prácticas y efectivas para nuestros clientes. En LATAMDATA, estamos comprometidos con el éxito de nuestros clientes.

- [Luis Gabriel Vela Coimbra]() - Data Engineer
- [Andres Felipe Corzo Angarita ]() - Data Engineer
- [Juan Carlos Contreras Figueredo ]() - Data Analyst
- [Agustin Balcaza]() - Data Analyst
- [David Andre Montes Saldarriaga ]() - Data Scientist

### Stack Tecnológico

<p align="center">
  <img src="./assets/Stack Tecnológico2.png" alt="Descripción de la imagen" width="500" height="400">
</p>

#### Fundamentacion

##### Python con bibliotecas como Pandas, Seaborn, NumPy, Matplotlib, Scikit-learn y Streamlit:

<p>Pandas: Es una biblioteca de manipulación y análisis de datos que proporciona estructuras de datos potentes y herramientas para trabajar con conjuntos de datos estructurados, facilitando tareas como limpieza, transformación y análisis.</p>
<p>NumPy: Proporciona estructuras de datos para trabajar con matrices multidimensionales y funciones matemáticas de alto nivel para operar con estas matrices. Es fundamental para operaciones numéricas eficientes.</p>
<p>Matplotlib: Es una librería de trazado de gráficos que permite generar visualizaciones estáticas, ofreciendo un alto grado de control sobre la apariencia de los gráficos.</p>
<p>Seaborn: Construida sobre Matplotlib, Seaborn proporciona una interfaz de alto nivel para crear visualizaciones estadísticas atractivas y detalladas. Es útil para la exploración rápida de datos y la creación de gráficos más complejos.</p>
<p>Scikit-learn: Ofrece herramientas para aprendizaje automático y modelado estadístico. Proporciona una gama de algoritmos y utilidades para tareas de clasificación, regresión, agrupamiento, entre otras.</p>
<p>Streamlit: Streamlit es una biblioteca de Python que permite crear aplicaciones web interactivas de manera rápida y sencilla. Está diseñado para facilitar a científicos de datos y desarrolladores la creación de interfaces de usuario para visualizar y compartir datos de manera efectiva. La premisa fundamental de Streamlit es proporcionar una experiencia de desarrollo minimalista, donde el código para crear aplicaciones sea simple y directo.</p>

##### Google Cloud Platform (GCP) con herramientas como Storage, Cloud Functions, BigQuery, Scheduler, App Engine y Cloud SDK:

<p>Google Cloud Storage: Ofrece almacenamiento escalable y seguro para objetos, permitiendo almacenar y acceder a grandes cantidades de datos de forma eficiente.</p>
<p>Google Cloud Functions: Permite ejecutar código en respuesta a eventos en la nube sin necesidad de aprovisionar o administrar servidores, ideal para aplicaciones serverless y microservicios.</p>
<p>BigQuery: Es un servicio de almacenamiento y análisis de datos totalmente administrado que permite consultar grandes conjuntos de datos usando SQL de forma rápida y escalable.</p>
<p>Cloud Scheduler: Cloud Scheduler es un servicio completamente administrado que te permite programar trabajos en GCP, de manera que puedas automatizar la ejecución de tareas recurrentes y planificadas. Algunos de los casos de uso comunes incluyen la ejecución de trabajos de procesamiento de datos, la activación de funciones en Cloud Functions y la actualización periódica de recursos en la nube.</p>
<p>App Engine: App Engine es un servicio de plataforma como servicio (PaaS) que te permite desarrollar y alojar aplicaciones web escalables sin preocuparte por la administración de la infraestructura subyacente. Es compatible con varios entornos de ejecución, incluidos Python, Java, Node.js y más.</p>
<p>Cloud SDK: Cloud SDK es un conjunto de herramientas de línea de comandos que facilitan la interacción con los productos y servicios de Google Cloud. Proporciona comandos para gestionar recursos, realizar despliegues y automatizar tareas administrativas en GCP.</p>

##### Power BI:

<p>Es una plataforma de análisis empresarial que permite visualizar datos y compartir información de manera interactiva y atractiva. Conecta con una variedad de fuentes de datos, incluyendo bases de datos en la nube o locales, y proporciona herramientas para crear informes dinámicos y cuadros de mando.</p>
<p>La combinación de estos componentes permite desde la manipulación y análisis avanzado de datos en Python, pasando por el almacenamiento y procesamiento en la nube con GCP, hasta la visualización interactiva y la generación de informes con Power BI. Esto facilita la creación de soluciones completas desde la adquisición y procesamiento de datos hasta la presentación y toma de decisiones basada en análisis detallados.</p>

### Workflow

Dada la prominencia que goza Google Cloud Platform (GCP), junto con el acceso a documentación actualizada y precisa, así como la extensa variedad de soluciones que proporciona en su plataforma, desde el almacenamiento y procesamiento de datos hasta el análisis predictivo y la visualización, hemos optado por elegirlo.

- Fuentes de Datos: Archivos planos + Webscrapping
- Integración: Cloud Functions en Google Cloud Platform
- Almacenamiento: Google Cloud Storage
- Data Warehouse: BigQuery en Google Cloud Platform
- Machine Learning: Google Cloud SDK + App Engine
- Visualización: Power Bi

### Pipeline

<p align="center">
  <img src="./assets/Pipeline Proyecto Google-Yelp.png" alt="Descripción de la imagen" width="650" height="450">
</p>

#### Descripcion

<p>En este proyecto, llevamos a cabo un proceso integral que abarca desde la obtención de datos hasta la visualización, utilizando diversas herramientas de Google Cloud Platform (GCP) y tecnologías de análisis de datos y aprendizaje automático.</p>
<p><strong>Obtención de Datos:</strong></p>
<p>Se obtienen conjuntos de datos de Google Maps y Yelp para alimentar nuestro análisis.</p>
<p><strong>ETL local con Python:</strong></p>
<p>Se realiza un proceso de Extracción, Transformación y Carga (ETL) de manera local utilizando Python y las bibliotecas pandas y numpy para limpiar y preparar los datos.</p>
<p><strong>EDA con Matplotlib y Seaborn:</strong></p>
<p>Se realiza un Análisis Exploratorio de Datos (EDA) utilizando las librerías Matplotlib y Seaborn para visualizar patrones y relaciones en los datos.</p>
<p><strong>Almacenamiento en GCP Storage:</strong></p>
<p>Se crea un bucket en la herramienta de Storage de GCP para almacenar los datos después de completar los procesos de ETL y EDA.</p>
<p><strong>ETL Automatizado con Cloud Functions y BigQuery:</strong></p>
<p>Se implementa un script en Cloud Functions para realizar un ETL automático, generando y eliminando columnas según sea necesario, y almacenando los resultados en BigQuery siguiendo un modelo de entidad-relación especificado en la carpeta Diagrama ER.</p>
<p><strong>Desarrollo del Modelo de Aprendizaje Automático:</strong></p>
<p>Se crea un archivo ML.ipynb para desarrollar un modelo de aprendizaje automático. Luego, se generan consultas y se realiza el despliegue local utilizando conda y los comandos especificados en el documento Readme_ML.md.</p>
<p><strong>Implementar en GCP App Engine:</strong></p>
<p>Se utiliza Cloud SDK para ejecutar comandos especificados en el documento Readme_ML.md, logrando así el despliegue del modelo como una aplicación que se almacena en GCP App Engine, proporcionando un enlace público para su consumo.</p>
<p><strong>Visualización con Power BI:</strong></p>
<p>Los datos almacenados en BigQuery se conectan a Power BI para la creación de Dashboards interactivos y visualmente atractivos.</p>
<p><strong>Web Scraping Automatizado con Funciones de la Nube:</strong></p>
<p>Se desarrolla un script en Cloud Functions para realizar web scraping de una página especificada en el documento Readme.md de la carpeta Google Cloud Platform. Se implementa un ETL automático y los datos se almacenan en BigQuery.</p>
<p><strong>Automatización con Cloud Scheduler:</strong></p>
<p>Se utiliza Cloud Scheduler para generar un trabajo cron anual que ejecuta el proceso de web scraping, garantizando la actualización periódica de los datos.</p>
<p>Este enfoque integrado aprovecha las capacidades de Google Cloud Platform para gestionar eficientemente cada etapa del proceso, desde la adquisición de datos hasta la implementación de modelos de aprendizaje automático y la visualización de resultados, proporcionando una solución robusta y escalable para análisis de datos avanzados.</p>


### Deploy ML

Para el Deploy del Modelo de Machine Learning, primerameramente se crea el modelo y se realiza la validacion usando las diferentes librerias el cual se especifica su proceso en el documento ´Readme_ML.md´, una vez hecho lo anterior esta listo para su deploy, que se hará en primer medida de manera local usando ´Streamlit´ libreria de Python. Para ello se crea el archivo ´app.py´ que contendra las funciones correspondientes para la ejecución del modelo y también se crea el archivo ´requeriments.txt´ el cual especificas las dependencias usadas dentro del archivo ´app.py´. Una vez se realizó lo anteriormente expuesto en la terminal de miniconda (terminal usada para este caso) se ejecutarán los siguientes comandos:

´$ conda create -n ApiProyecto´    "Nota: Se aclara que para este caso ApiProyecto es el nombre de nuestro entorno virtual creado."
´$ conda activate ApiProyecto´     "Este comando activate el entorno virtual anteriormente creado."
´$ conda install python=3.10.12´   "Este comando instala la version de Python en el entorno virtual."
´$ pip install -r requeriments.txt´ " Este comando instala las dependecias contenidas en el archivo requeriments.txt."
´$ streamlit run app.py´           "Este comando deploya la aplicación contenida en el archivo ´app.py´."

Para el proceso de producción del Modelo de Machine Learning se utiliza las herramientas de Cloud SDK y App Engine y todo su proceso se especifica en el documento ´Readme.md´ contenido en la carpeta ´Google Cloud Platform´.

Nota: Los archivos utilizados en el deploy de manera local y en la producción se encuentra en la subcarpeta ´Cloud SDK y App Engine´ contenida en la carpeta de ´Google Cloud Platform´.
